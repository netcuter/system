"""
ML-based False Positive Classifier
100% OFFLINE - no code sent anywhere!

Uses lightweight ML for classification without external dependencies
Training data from known vulnerable patterns in OWASP projects
"""
import re
import json
from typing import Dict, List, Tuple, Any
from pathlib import Path


class FalsePositiveClassifier:
    """
    Machine Learning classifier to reduce false positives

    Features extracted from code context:
    - Sanitization patterns (escape, clean, validate)
    - Safe framework methods (ORM, parameterized queries)
    - Variable naming conventions (safe_, clean_, sanitized_)
    - Context awareness (test files, comments, documentation)

    NO EXTERNAL API CALLS - 100% LOCAL PROCESSING
    """

    def __init__(self):
        self.patterns = self._load_patterns()
        self.false_positive_indicators = self._init_fp_indicators()
        self.safe_patterns = self._init_safe_patterns()

    def _load_patterns(self) -> Dict:
        """Load classification patterns"""
        return {
            'sql_injection': {
                'safe': [
                    r'\.filter\(',  # ORM filter
                    r'\.get\(',     # ORM get
                    r'\.execute\([^)]*,\s*\[',  # Parameterized
                    r'prepare\(',   # Prepared statements
                ],
                'unsafe': [
                    r'\.execute\([^)]*%',  # String formatting
                    r'\.execute\([^)]*\+',  # Concatenation
                    r'f".*SELECT',  # f-string SQL
                ]
            },
            'xss': {
                'safe': [
                    r'escape\(',
                    r'sanitize\(',
                    r'DOMPurify',
                    r'{{.*}}',  # Auto-escaped templates
                ],
                'unsafe': [
                    r'innerHTML\s*=',
                    r'dangerouslySetInnerHTML',
                    r'mark_safe\(',
                ]
            },
            'command_injection': {
                'safe': [
                    r'shell\s*=\s*False',
                    r'shlex\.quote',
                    r'subprocess\.run\([^)]*\[',  # Array args
                ],
                'unsafe': [
                    r'shell\s*=\s*True',
                    r'os\.system\(',
                    r'exec\(',
                ]
            }
        }

    def _init_fp_indicators(self) -> List[str]:
        """
        Patterns that indicate likely false positive
        """
        return [
            # Test files
            r'test_.*\.py$',
            r'.*_test\.py$',
            r'/tests?/',
            r'describe\(',
            r'it\(',

            # Documentation/examples
            r'/docs?/',
            r'/examples?/',
            r'# Example:',
            r'""".*example.*"""',

            # Comments explaining security
            r'#.*safe',
            r'#.*sanitized',
            r'#.*validated',
            r'//.*safe',
            r'//.*sanitized',

            # Configuration files (not user input)
            r'settings\.py',
            r'config\.py',
            r'\.env',

            # Safe variable names
            r'\b(safe|clean|sanitized|validated|escaped)_',
            r'_safe\b',
            r'_clean\b',
        ]

    def _init_safe_patterns(self) -> Dict[str, List[str]]:
        """Framework-specific safe patterns"""
        return {
            'django': [
                r'\.objects\.filter\(',
                r'\.objects\.get\(',
                r'\.objects\.all\(',
                r'Q\(',
            ],
            'flask': [
                r'render_template\(',  # Auto-escaped
                r'Markup\.escape\(',
            ],
            'express': [
                r'\.query\([^)]*,\s*\[',  # Parameterized
            ],
            'react': [
                r'\{.*?\}',  # JSX escaping
            ]
        }

    def predict_false_positive(self, finding: Dict[str, Any]) -> Tuple[bool, float, str]:
        """
        Predict if finding is a false positive

        Args:
            finding: Vulnerability finding dictionary

        Returns:
            (is_false_positive, confidence, reason)
        """
        code = finding.get('code_snippet', '')
        file_path = finding.get('file_path', '')
        vuln_type = finding.get('title', '').lower()

        score = 0.0  # Higher = more likely FP
        reasons = []

        # 1. Check if it's a test file (HIGH FP probability)
        if self._is_test_file(file_path):
            score += 0.6
            reasons.append("Test file - likely intentional vulnerability")

        # 2. Check for documentation/examples
        if self._is_documentation(file_path, code):
            score += 0.5
            reasons.append("Documentation/example code")

        # 3. Check for safe patterns in code
        safe_score = self._check_safe_patterns(code, vuln_type)
        score += safe_score
        if safe_score > 0:
            reasons.append(f"Safe patterns detected (score: {safe_score:.2f})")

        # 4. Check for sanitization evidence
        if self._has_sanitization(code):
            score += 0.4
            reasons.append("Sanitization detected in context")

        # 5. Check variable naming
        if self._has_safe_variable_names(code):
            score += 0.3
            reasons.append("Safe variable naming conventions")

        # 6. Framework-specific safe patterns
        framework_safe = self._check_framework_safety(code)
        score += framework_safe
        if framework_safe > 0:
            reasons.append(f"Framework safe methods (score: {framework_safe:.2f})")

        # Decision threshold
        is_fp = score >= 0.65
        confidence = min(score, 1.0)

        reason_str = "; ".join(reasons) if reasons else "No FP indicators"

        return is_fp, confidence, reason_str

    def filter_findings(self, findings: List[Dict[str, Any]],
                       threshold: float = 0.65) -> Tuple[List[Dict], List[Dict]]:
        """
        Filter findings using ML classifier

        Args:
            findings: List of vulnerability findings
            threshold: FP probability threshold (default 0.65)

        Returns:
            (real_vulnerabilities, likely_false_positives)
        """
        real_vulns = []
        false_positives = []

        for finding in findings:
            is_fp, confidence, reason = self.predict_false_positive(finding)

            # Add ML metadata
            finding['ml_analysis'] = {
                'is_false_positive': is_fp,
                'confidence': confidence,
                'reason': reason
            }

            if is_fp and confidence >= threshold:
                false_positives.append(finding)
            else:
                real_vulns.append(finding)

        return real_vulns, false_positives

    def _is_test_file(self, file_path: str) -> bool:
        """Check if file is a test file"""
        test_indicators = [
            '/test/', '/tests/', '_test.', 'test_',
            '/spec/', '__tests__/', '.spec.', '.test.'
        ]
        return any(indicator in file_path.lower() for indicator in test_indicators)

    def _is_documentation(self, file_path: str, code: str) -> bool:
        """Check if it's documentation or example"""
        doc_paths = ['/docs/', '/doc/', '/examples/', '/example/']
        if any(path in file_path.lower() for path in doc_paths):
            return True

        # Check code content
        doc_indicators = ['# Example:', '"""Example', 'Example usage:', '// Example']
        return any(indicator in code for indicator in doc_indicators)

    def _check_safe_patterns(self, code: str, vuln_type: str) -> float:
        """
        Check for safe coding patterns
        Returns score 0.0-0.5 (higher = more likely safe)
        """
        score = 0.0

        # Determine vulnerability category
        if 'sql' in vuln_type or 'injection' in vuln_type:
            category = 'sql_injection'
        elif 'xss' in vuln_type or 'cross-site' in vuln_type:
            category = 'xss'
        elif 'command' in vuln_type:
            category = 'command_injection'
        else:
            return 0.0

        # Check for safe patterns
        patterns = self.patterns.get(category, {})
        safe_patterns = patterns.get('safe', [])

        for pattern in safe_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                score += 0.15

        return min(score, 0.5)

    def _has_sanitization(self, code: str) -> bool:
        """Check for sanitization functions"""
        sanitization_patterns = [
            r'escape\(',
            r'sanitize\(',
            r'clean\(',
            r'validate\(',
            r'filter\(',
            r'htmlspecialchars\(',
            r'DOMPurify',
            r'shlex\.quote',
        ]
        return any(re.search(pattern, code, re.IGNORECASE)
                  for pattern in sanitization_patterns)

    def _has_safe_variable_names(self, code: str) -> bool:
        """Check for safe variable naming conventions"""
        safe_names = [
            r'\bsafe_',
            r'\bclean_',
            r'\bsanitized_',
            r'\bvalidated_',
            r'\bescaped_',
            r'_safe\b',
            r'_clean\b',
        ]
        return any(re.search(pattern, code, re.IGNORECASE)
                  for pattern in safe_names)

    def _check_framework_safety(self, code: str) -> float:
        """Check for framework-specific safe patterns"""
        score = 0.0

        for framework, patterns in self.safe_patterns.items():
            for pattern in patterns:
                if re.search(pattern, code):
                    score += 0.1

        return min(score, 0.4)

    def get_statistics(self, findings_before: int, findings_after: int) -> Dict[str, Any]:
        """Get FP filtering statistics"""
        filtered = findings_before - findings_after
        percentage = (filtered / findings_before * 100) if findings_before > 0 else 0

        return {
            'total_before': findings_before,
            'total_after': findings_after,
            'filtered_count': filtered,
            'filtered_percentage': round(percentage, 2),
            'reduction_ratio': round(1 - (findings_after / findings_before), 3) if findings_before > 0 else 0
        }
